{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASTR 3890 - Selected Topics: Data Science for Large Astronomical Surveys (Spring 2022)\n",
    "***N. Hernitschek***\n",
    "___\n",
    "\n",
    "# Final Exam\n",
    "### Due: Monday, May 2nd at 4.00pm CST\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "A model with 10 parameters describes the statistics of the temperature fluctuations\n",
    "of the Cosmic Microwave Background. We fit this model to 45 published data points of the\n",
    "variance of the fluctuations as a function of angular scale and find χ2 = 52.7. We then set 5\n",
    "of the model parameters to zero and fit the model again, this time getting χ2 = 54. Were we\n",
    "justified in using all 10 parameters in the model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greater difference between expected and actual data produces greater $\\chi^{2}$ value. Here we can see that when we are using 10 parameters, we are getting less $\\chi^{2}$ value. So, using 10 parameters would give us a better expected data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Explain what Malmquist bias is. Use the Malmquist bias to explain how it can fool the unwary astronomer into thinking that stellar or galaxy luminosities measured at different wavelengths can appear correlated, even if there is no true correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malmquist bias is a selection bias applicable to astronomical surveys stemming from missing the dimmest objects. The further away, the larger percentage of objects at that distance will be too dim to be detected, and the collected observations will include both bright and dim objects at nearer distances but only the brighter ones at greater distances. In other words, the greater the distance, the stronger the preference (bias) toward brighter objects, meaning evaluation of the randomness of sample-sets must take this into consideration. Generally the brightness is associated with the type or size of the object, and population statistics at different distances need to take the bias into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "1) Read in final_data_3.npy. This is a (10 x 2) numpy array, with voltage measurements in the first column and heteroscedastic voltage uncertainties in the second column. Compute the sample mean and the standard error on the sample mean for this data.\n",
    "\n",
    "2) Fit the appropriate ln-likelihood function and find the best-fit mean voltage.\n",
    "\n",
    "3) Compute and plot the Bayesian posterior probability density (not the log posterior) for the mean voltage assuming a uniform prior for the mean in the range 3 to 7. Make sure this posterior pdf is normalized.\n",
    "\n",
    "4) By either drawing samples from this posterior, or using your gridded posterior pdf to make a cdf, find the equal-tailed 68.3% credible region for the mean, and compare the upper and lower boundaries to the sample mean plus/minus the standard error, respectively. Also find the MAP value of the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the data is 3.3000000000000003 .\n",
      "The sem of the data is 1.0599999999999998 .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import mean\n",
    "from scipy.stats import sem\n",
    "\n",
    "data = np.load('final_data_3.npy')\n",
    "\n",
    "\n",
    "mean = np.mean(data[0])\n",
    "\n",
    "print(\"The mean of the data is\", mean, \".\")\n",
    "\n",
    "sem = sem(data[0])\n",
    "print(\"The sem of the data is\", sem, \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "This problem has to do with dimensional reduction. We're going to load in a sample of SDSS Imaging data.\n",
    "\n",
    "1. Execute the cell below to read in the data, print out the feature names, and create a data matrix out of a subset of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ra', 'dec', 'run', 'rExtSFD', 'uRaw', 'gRaw', 'rRaw', 'iRaw', 'zRaw', 'uErr', 'gErr', 'rErr', 'iErr', 'zErr', 'uRawPSF', 'gRawPSF', 'rRawPSF', 'iRawPSF', 'zRawPSF', 'upsfErr', 'gpsfErr', 'rpsfErr', 'ipsfErr', 'zpsfErr', 'type', 'ISOLATED')\n"
     ]
    }
   ],
   "source": [
    "from astroML.datasets import fetch_imaging_sample\n",
    "data = fetch_imaging_sample()  \n",
    "data.shape  # number of objects in dataset\n",
    "\n",
    "print(data.dtype.names)\n",
    "\n",
    "keylist = ['ra', 'dec', 'rExtSFD', 'uRaw', \n",
    "           'gRaw', 'rRaw', 'iRaw', 'zRaw', \n",
    "           'uErr', 'gErr', 'rErr', 'iErr', \n",
    "           'zErr', 'uRawPSF', 'gRawPSF', \n",
    "           'rRawPSF', 'iRawPSF', 'zRawPSF', \n",
    "           'upsfErr', 'gpsfErr', 'rpsfErr', \n",
    "           'ipsfErr', 'zpsfErr']\n",
    "\n",
    "X = np.column_stack([data[key] for key in keylist]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use PCA (with randomized svd_solver for speed if necessary) to reduce the data matrix down to features. What is the explained variance of the data encapsulated in these eigen-features?\n",
    "\n",
    "3. \n",
    "    Access the `type` key of the `data` structure and make an array of labels out of these. Do some research in astroML documentation to find out what these integer types correspond to, and state that here.\n",
    "\n",
    "4. Choose 5000 random integers between 0 and the number of samples in the data matrix. Record these integers because you'll use them later. Make a scatter plot of the PCA-reduced data for these 5000 random samples, colored by their corresponding type. (You may want to set the transparency to be lower than 1 to see the mixing of samples.)\n",
    "\n",
    "\n",
    "5.  Now try some non-linear dimensional reduction. These algorithms are slower than PCA, so you will operate only on the 5000 random samples identified in the previous part.\n",
    "\n",
    "* Try LocallyLinearEmbedding, Isomap, and TSNE algorithms, setting the number of components to be 2 in all cases.\n",
    "* As in the PCA case, make scatter plots of the dimensionally-reduced data, color coded by their type. For LLE and Isomap, experiment with the number of nearest neighbors between 5 and 100 to see what visually gives the best separation in type populations. For TSNE, do the same for the perplexity attribute.\n",
    "* Which algorithm gives the cleanest way to visually see the two populations of sources? (This will be subjective according to the samples you trained on, and even the randomness of the algorithms.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
